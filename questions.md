Answer Following Questions in detail.

    What is Machine Learning ?
    
   Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers learn automatically without human intervention or assistance and adjust actions accordingly.Eg. Machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome.
    
    What is Data Science ?
    
   Data science, also known as data-driven science, is an interdisciplinary field about scientific methods, processes, and systems to extract knowledge or insights from data in various forms, either structured or unstructured, similar to data mining.
It is a multidisciplinary blend of data inference, algorithm development, and technology in order to solve analytically complex problems.

    What is Data Mining ?
    
   Data mining is the practice of automatically searching large stores of data to discover patterns and trends that go beyond simple analysis. Data mining uses sophisticated mathematical algorithms to segment the data and evaluate the probability of future events. Data mining is also known as Knowledge Discovery in Data (KDD).Data mining tools allow enterprises to predict future trends.In other words, we can say that data mining is the procedure of mining knowledge from data.
 
    What is Data Analytics ?
    
   Data Analytics is the science of examining raw data with the purpose of finding patterns and drawing conclusions about that information by applying an algorithmic or mechanical process to derive insights with the aid of specialized systems and software.
Data analytics technologies and techniques are widely used in commercial industries to enable organizations to make more-informed business decisions and by scientists and researchers to verify or disprove scientific models, theories and hypotheses.
    
     What are the tools used for the Data Mining ?
     
1.   WEKA
2.   Rapid Miner
3.   Orange
4.   Knime
5.   DataMelt
6.   Apache Mahout
7.   ELKI
8.   MOA
9.   KEEL
10. Rattle
     
    Explain the Limitations of the tools which you have listed in the above questions answer.
    
1. WEKA

Weka is a Java based free and open source software licensed under the GNU GPL and available for use on Linux, Mac OS X and Windows. It comprises a collection of machine learning algorithms for data mining. It packages tools for data pre-processing, classification, regression, clustering, association rules and visualisation. It lets you import the raw data from various file formats, and supports well known algorithms for different mining actions like filtering, clustering, classification and attribute selection. Weka has proved to be an ideal choice for educational and research purposes, as well as for rapid prototyping.

2. Rapid Miner

This is very popular since it is a readymade, open source, no-coding required software, which gives advanced analytic s. Written in Java, it incorporates multifaceted data mining functions such as data pre-  processing, visualization, predictive analysis, and can be easily integrated with WEKA and R-tool to directly give models from scripts written in the former two. The tool is also compatible with weak scripts. Rapid Miner is used for business/commercial applications, research and education.

3. Orange

Orange is a Python library that powers Python scripts with its rich compilation of mining and machine learning algorithms for data pre-	processing, classification, modelling, regression, clustering and other miscellaneous functions. Orange also comes with a visual programming environment and its workbench consists of tools for importing data, and dragging and dropping widgets and links to connect different widgets for completing the workflow. Due to the ease of programming and integration in Python, Orange can be a great take off point for novices and experts to plunge into data mining.



4. Knime

Knime is one of the leading open source analytic, integration and 	reporting platforms that comes as free software and as well as a commercial version. Written in Java and built upon Eclipse, its access is 	through a GUI that provides options to create the data flow and conduct data pre-processing, collection, analysis, modelling and reporting. Customers are happy with the platform’s flexibility, openness and smooth 	integration with other software like Weka and R. Given the small size of the company, Knime has a large user base and an active community. It 	makes use of Eclipse’s extension mechanism capability to add plugins for 	the required functionalities like text and image mining. This software is ideal for enterprise use.



5. DataMelt

DataMelt is a computational platform, offering statistics, numeric and 	symbolic computations, scientific visualisation, etc.DMelt provides data 	mining features like linear regression, curve fitting, cluster analysis, 	neural networks, fuzzy algorithms, analytic calculations and interactive 	visualisations using 2D/3D plots and histograms. One can play around 	with its IDE (integrated development kit) or its functions can be called 	from applications using its Java API. Both community and commercial 	editions of DMelt are available on Linux, Mac OS, Windows and Android 	platforms. DMelt is a successor to the jHepWork and SCaVis programs, 	which some people working in data analysis might be familiar with. This 	software is well suited for students, engineers and scientists.





6. Apache Mahout

Mahout is primarily a library of machine learning algorithms that can help 	in clustering, classification and frequent pattern mining. It can be used in 	a distributed mode that helps easy integration with Hadoop. Mahout is 	currently being used by some of the giants in the tech industry like 	Adobe, AOL, Drupal and Twitter, and it has also made an impact in research and academics. It can be a great choice for anyone looking for easy integration with Hadoop and to mine huge volumes of data.

7. ELKI

ELKI is open source software written in Java and licensed under AGPLv3. This software focuses especially on cluster analysis and outlier detection with a compilation of numerous algorithms from both these 	domains. The software is accessed through a GUI that displays the results once the selected algorithm is run. ELKI’s design goals are performance, scalability, completeness, extensibility and a modular design to welcome contributions. ELKI currently doesn’t offer professional support and the software is optimised for use in science and research. Hence, this option works best for those in research.

8. MOA

Massive Online Analysis (MOA), as the name suggests, is primarily data stream mining software that is well suited for applications that need to handle volumes of real-time data streams at a high speed. MOA is distributed under GNU GPL, and can be used via the command line, GUI or Java API. It is a rich compilation of machine learning algorithms and has proved to be a great choice during the design of real-time applications. MOA is well suited for these requirements. Weka and MOA can be closely linked to each other and either of the classifiers can be called from the other one. For those looking to analyse and mine information from real-time data, MOA can be the best choice.




9. KEEL

KEEL (Knowledge Extraction for Evolutionary Learning) is a Java based open source tool distributed under GPLv3. It is powered by a well-organised GUI that lets you manage (import, export, edit and visualise) data with different file formats, and to experiment with the data (through its data pre-processing, statistical libraries and some standard data mining and evolutionary learning algorithms). Since KEEL is based on Java, JVM has to be installed on the system to run its GUI and do data mining experiments. You may visit http://keel.es/ for the complete list of supported algorithms. KEEL is ideal for research and educational purposes. It serves as a useful aid for teachers.




10. Rattle

Rattle, expanded to ‘R Analytical Tool To Learn Easily’, has been developed using the R statistical programming language. The software can run on Linux, Mac OS and Windows, and features statistics, 	clustering, modelling and visualisation with the computing power of R. 	Rattle is currently being used in business, commercial enterprises and for 	teaching purposes in Australian and American universities.
    
    What are the tasks in the Machine Learning ?
    
Regression
Classification
Clustering
Multivariate querying
Density estimation
Dimension reduction
Testing and matching
    
    Give at least 5 examples of the tasks which you have given in above question.
    
Regression: Regression tasks mainly deal with estimation of numerical values (continuous variables). Some of the examples include estimation of housing price, product price, stock price etc. Some of the following ML methods could be used for solving regressions problems:
a. Kernel regression (Higher accuracy)
b. Gaussian process regression (Higher accuracy)
c. Regression trees
d. Linear regression
e. Support vector regression

Classification: Classification tasks is simply related with predicting a category of a data (discrete variables). One of the most common example is predicting whether or not an email if spam or ham. Some of the common usecases could be found in the area of healthcare such as whether a person is suffering from a particular disease or not. It also has its application in financial usecases such as determining whether a transaction is fraud or not. The ML methods such as following could be applied to solve classification tasks:
f. Kernel discriminant analysis (Higher accuracy)
g. K-Nearest Neighbors (Higher accuracy)
h. Artificial neural networks (ANN) (Higher accuracy)
i. Support vector machine (SVM) (Higher accuracy)
j. Random forests (Higher accuracy)

Clustering: Clustering tasks are all about finding natural groupings of data and a label associated with each of these groupings (clusters). Some of the common example includes customer segmentation, product features identification for product roadmap. Some of the following are common ML methods:
k. Mean-shift  (Higher accuracy)
l. Hierarchical clustering
m. K-means
n. Topic models

Multivariate querying: Multivariate querying is about querying or finding similar objects. Some of the following ML methods could be used for such problems:
o. Nearest neighbors
p. Range search
q. Farthest neighbors

Density estimation: Density estimation problems are related with finding likelihood or frequency of objects. In probability and statistics, density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function. Some of the following ML methods could be used for solving density estimation tasks:
r. Kernel density estimation (Higher accuracy)
s. Mixture of Gaussians
t. Density estimation tree
    
     Do a research of what is the impact of Machine Learning on the society
     
Overview:
Advances in deep learning and other machine learning algorithms are currently causing a tectonic shift in the technology landscape. Technology behemoths like Google, Microsoft, Amazon, Facebook and Salesforce are engaged in an artificial intelligence (AI) arms race, gobbling up machine learning talent and startups at an alarming pace. They are building AI technology war chests in an effort to develop an insurmountable competitive advantage.
     
     Research for privacy and machine learning.
  
Overview
Since the dawn of big data, privacy concerns have overshadowed every advancement and every new algorithm. This is the same for machine learning, which learns from big data to essentially think for itself. This presents an entirely new threat to privacy, opening up volumes of data for analysis on a whole new scale. Many standard applications of machine learning and statistics will, by default, compromise the privacy of individuals represented in the data sets. They are also vulnerable to hackers, who would edit the training data, compromising both the data and the final goal of the algorithm.
  
 
